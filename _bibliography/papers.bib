---
---
@article{saycanpay,
    author = {Hazra, Rishi and Zuidberg Dos Martires, Pedro and De Raedt, Luc},
    title = {SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge},
    journal = {Association for Advancement of Artificial Intelligence (AAAI)},
    year = {2024},
    abbr = {AAAI 2024},
    selected={true},
    pdf = {https://arxiv.org/pdf/2308.12682.pdf},
    website = {https://rishihazra.github.io/SayCanPay/},
    abstract = {Large Language Models (LLMs) have demonstrated impressive planning abilities due to their vast world knowledge. Yet, obtaining plans that are both feasible (grounded in affordances) and cost-effective (in plan length), remains a challenge, despite recent progress. This contrasts with heuristic planning methods that employ domain knowledge (formalized in action models such as PDDL) and heuristic search to generate feasible, optimal plans. Inspired by this, we propose to combine the power of LLMs and heuristic planning by leveraging the world knowledge of LLMs and the principles of heuristic search. Our approach, SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain knowledge, that evaluates actions’ feasibility (Can) and long-term reward/payoff (Pay), and heuristic search to select the best sequence of actions. Our contributions are (1) a novel framing of the LLM planning problem in the context of heuristic planning, (2) integrating grounding and cost-effective elements into the generated plans, and (3) using heuristic search over actions. Our extensive evaluations show that our model surpasses other LLM planning approaches.},
    code = {https://github.com/RishiHazra/saycanpay},
    preview = {saycanpay-gif.gif},
}

@article{hazra2023egotv,
  title={EgoTV: Egocentric Task Verification from Natural Language Task Descriptions},
  author={Hazra, Rishi and Chen, Brian and Rai, Akshara and Kamra, Nitin and Desai, Ruta},
  journal={International Conference of Computer Vision (ICCV)},
  year={2023},
  abbr={ICCV 2023},
    selected={true},
    preview = {egotv-pos.gif},
    pdf = {https://arxiv.org/pdf/2303.16975.pdf},
    code = {https://github.com/facebookresearch/EgoTV},
    website = {https://rishihazra.github.io/EgoTV},
    abstract = {To enable progress towards egocentric agents capable of understanding everyday tasks specified in natural language, we propose a benchmark and a synthetic dataset called Egocentric Task Verification (EgoTV). EgoTV contains multi-step tasks with multiple sub-task decompositions, state changes, object interactions, and sub-task ordering constraints, in addition to abstracted task descriptions that contain only partial details about ways to accomplish a task. We also propose a novel Neuro-Symbolic Grounding (NSG) approach to enable the causal, temporal, and compositional reasoning of such tasks. We demonstrate NSG's capability towards task tracking and verification on our EgoTV dataset and a real-world dataset derived from CrossTask (CTV). Our contributions include the release of the EgoTV and CTV datasets, and the NSG model for future research on egocentric assistive agents.},
}

@article{hazra2023derrl,
      title={Deep Explainable Relational Reinforcement Learning: A Neuro-Symbolic Approach},
      author={Hazra, Rishi and De Raedt, Luc},
      year={2023},
      journal={European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)},
      abbr = {ECML PKDD 2023},
    selected={true},
      preview = {derrl-all-envs.png},
    pdf = {https://arxiv.org/pdf/2304.08349.pdf},
    abstract = {Despite numerous successes in Deep Reinforcement Learning (DRL), the learned policies are not interpretable. Moreover, since DRL does not exploit symbolic relational representations, it has difficulties in coping with structural changes in its environment (such as increasing the number of objects). Relational Reinforcement Learning, on the other hand, inherits the relational representations from symbolic planning to learn reusable policies. However, it has so far been unable to scale up and exploit the power of deep neural networks. We propose Deep Explainable Relational Reinforcement Learning (DERRL), a framework that exploits the best of both -- neural and symbolic worlds. By resorting to a neuro-symbolic approach, DERRL combines relational representations and constraints from symbolic planning with deep learning to extract interpretable policies. These policies are in the form of logical rules that explain how each decision (or action) is arrived at. Through several experiments, in setups like the Countdown Game, Blocks World, Gridworld, and Traffic, we show that the policies learned by DERRL can be applied to different configurations and contexts, hence generalizing to environmental modifications.},
    slides = {DERRL.pptx},
}

@misc{hazra2023intrinsically,
      title={Intrinsically Motivated Compositional Language Emergence},
      author={Rishi Hazra and Sonu Dixit and Sayambhu Sen},
      year={2023},
      journal={arXiv preprint},
      abbr={preprint},
      selected = {true},
      pdf={https://arxiv.org/pdf/2012.05011.pdf},
    preview = {push.gif},
    abstract = {Recently, there has been a great deal of research in emergent communication on artificial agents interacting in simulated environments. Recent studies have revealed that, in general, emergent languages do not follow the compositionality patterns of natural language. To deal with this, existing works have proposed a limited channel capacity as an important constraint for learning highly compositional languages. In this paper, we show that this is not a sufficient condition and propose an intrinsic reward framework for improving compositionality in emergent communication. We use a reinforcement learning setting with two agents -- a task-aware Speaker and a state-aware Listener that are required to communicate to perform a set of tasks. Through our experiments on three different referential game setups, including a novel environment gComm, we show intrinsic rewards improve compositionality scores by ~1.5−2 times that of existing frameworks that use limited channel capacity.},
    website = {https://sites.google.com/view/compositional-comm},
}

@article{hazra2021gcomm,
  title={gComm: An environment for investigating generalization in Grounded Language Acquisition},
  author={Hazra*, Rishi and Dixit*, Sonu},
  journal={Fourth workshop on Visually Grounded Interaction and Language (ViGIL)},
  year={2021},
  pdf={https://vigilworkshop.github.io/static/papers-2021/16.pdf},
  abbr={ViGIL '21},
     preview = {gcomm.png},
    selected={true},
    code = {https://github.com/SonuDixit/gComm},
    poster = {https://drive.google.com/file/d/1b512g822Z-h0PvJkmO8y8YwhkDUNDHeG/view?usp=sharing},
    abstract = {gComm is a step towards developing a robust platform to foster research in grounded language acquisition in a more challenging and realistic setting. It comprises a 2-d grid environment with a set of agents (a stationary speaker and a mobile listener connected via a communication channel) exposed to a continuous array of tasks in a partially observable setting. The key to solving these tasks lies in agents developing linguistic abilities and utilizing them for efficiently exploring the environment. The speaker and listener have access to information provided in different modalities, i.e. the speaker's input is a natural language instruction that contains the target and task specifications and the listener's input is its grid-view. Each must rely on the other to complete the assigned task, however, the only way they can achieve the same, is to develop and use some form of communication. gComm provides several tools for studying different forms of communication and assessing their generalization.},
}

@inproceedings{hazra-etal-2021-active2,
    title = "Active$^2$ Learning: Actively reducing redundancies in Active Learning methods for Sequence Tagging and Machine Translation",
    author = "Hazra, Rishi  and
      Dutta, Parag  and
      Gupta, Shubham  and
      Qaathir, Mohammed Abdul  and
      Dukkipati, Ambedkar",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pdf = "https://aclanthology.org/2021.naacl-main.159",
    doi = "10.18653/v1/2021.naacl-main.159",
    pages = "1982--1995",
    abbr = "NAACL '21",
    selected={true},
    preview = {a2l.png},
    code = {https://github.com/parag1604/A2L},
    poster = {https://drive.google.com/file/d/1Z-mfqOTaF348oMPICygd1QA8kFc4ppkv/view?usp=sharing},
    abstract = {While deep learning is a powerful tool for natural language processing (NLP) problems, successful solutions to these problems rely heavily on large amounts of annotated samples. However, manually annotating data is expensive and time-consuming. Active Learning (AL) strategies reduce the need for huge volumes of labeled data by iteratively selecting a small number of examples for manual annotation based on their estimated utility in training the given model. In this paper, we argue that since AL strategies choose examples independently, they may potentially select similar examples, all of which may not contribute significantly to the learning process. Our proposed approach, Active2 Learning (A2L), actively adapts to the deep learning model being trained to eliminate such redundant examples chosen by an AL strategy. We show that A2L is widely applicable by using it in conjunction with several different AL strategies and NLP tasks. We empirically demonstrate that the proposed approach is further able to reduce the data requirements of state-of-the-art AL strategies by ≈ 3-25% on an absolute scale on multiple NLP tasks while achieving the same performance with virtually no additional computation overhead.},
}

@inproceedings{10.5555/3398761.3399006,
author = {Gupta*, Shubham and Hazra*, Rishi and Dukkipati, Ambedkar},
title = {Networked Multi-Agent Reinforcement Learning with Emergent Communication},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We develop a Multi-Agent Reinforcement Learning (MARL) method that finds approximately optimal policies for cooperative agents that co-exist in an environment. Central to achieving this is how the agents learn to communicate with each other. Can they together develop a language while learning to perform a common task? We formulate and study a MARL problem where cooperative agents are connected via a fixed underlying network. These agents communicate along the edges of this network by exchanging discrete symbols. However, the semantics of these symbols are not predefined and have to be learned during the training process. We propose a method for training these agents using emergent communication. We demonstrate the applicability of the proposed framework by applying it to the problem of managing traffic controllers, where we achieve state-of-the-art performance (as compared to several strong baselines) and perform a detailed analysis of the emergent communication.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)},
pages = {1858–1860},
numpages = {3},
keywords = {multi-agent reinforcement learning, emergent communication, traffic},
location = {Auckland, New Zealand},
pdf = {https://arxiv.org/pdf/2004.02780.pdf},
abbr = {AAMAS '20},
    selected={true},
     preview = {word_action_whole.png}
}