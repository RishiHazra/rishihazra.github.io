<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://rishihazra.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rishihazra.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-08T04:54:14+00:00</updated><id>https://rishihazra.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Why are LLMs required for Planning?</title><link href="https://rishihazra.github.io/llm-planning/2024/05/26/llms_as_planners.html" rel="alternate" type="text/html" title="Why are LLMs required for Planning?"/><published>2024-05-26T15:06:00+00:00</published><updated>2024-05-26T15:06:00+00:00</updated><id>https://rishihazra.github.io/llm-planning/2024/05/26/llms_as_planners</id><content type="html" xml:base="https://rishihazra.github.io/llm-planning/2024/05/26/llms_as_planners.html"><![CDATA[<p>In my talks, I often encounter a couple of recurring questions. The first is <strong>Why</strong> do I use LLMs as Planners in my works? Is it simply because they‚Äôre the latest rage. Let‚Äôs be clear ‚Äì if you can do something with classical planners ‚Äì <strong>don‚Äôt</strong> use a trillion parameter model for the same? That‚Äôd be like throwing a kitchen sink at it. However, as we will see, not everything can be <em>simply</em> solved using a classical planner.</p> <p>Once we tackle that, the inevitable follow-up question arises <strong>Are they all you need for planning</strong>?</p> <p>These are common questions, and many of us don‚Äôt have clear answers. Through this blog post, I aim to shed light on these inquiries. To kick things off, let‚Äôs clearly outline the key questions we‚Äôll explore:</p> <p><strong>Q1. Why are LLMs useful for planning?</strong> Why consider LLMs over classical methods? We‚Äôll dive into the flexibility and dynamic problem-solving capabilities of LLMs, illustrating how they adapt to new and unforeseen challenges in planning.</p> <p><strong>Q2. Are they really All You Need <em>for planning</em>?</strong> Can LLMs meet all your planning needs, or do they have their limitations? We‚Äôll critically assess the strengths and potential drawbacks of relying solely on LLMs for planning tasks.</p> <hr/> <h2 id="q1-why-are-llms-useful-for-planning">Q1. Why are LLMs useful for planning?<br/><br/></h2> <h3 id="1-open-world-planning">1. Open-world Planning<br/><br/></h3> <div class="row"> <div class="col-sm-12 mt-3 mt-md-0"> <img src="/assets/img/open-world-planning-1.png" width="70%" alt="Description of the image content" class="img-fluid rounded z-depth-1" style="display: block; margin: auto;" onerror="this.onerror=null; this.src='image-not-found.png';"/> </div> </div> <p><br/><br/></p> <p>Here‚Äôs a scenario: a robot is tasked to deliver a cup for a drink, but the cup is already occupied by tableware. If the robot can‚Äôt empty the cup itself, what‚Äôs it to do? Stuck with a predefined domain, a classical planner is restricted to the predefined objects (cups and glasses) it in its <a href="https://planning.wiki/ref/pddl/domain">domain file</a>.</p> <p>A LLM can draw from its world knowledge to suggest, ‚ÄúHow about a bowl?‚Äù<a href="#1">[1]</a> As humans, we know a bowl can function much like a cup ‚Äî in technical terms, they have similar <em>affordances</em>. <br/><br/></p> <div class="row"> <div class="col-sm-12 mt-3 mt-md-0"> <img src="/assets/img/open-world-planning-2.png" width="70%" alt="Description of the image content" class="img-fluid rounded z-depth-1" style="display: block; margin: auto;" onerror="this.onerror=null; this.src='image-not-found.png';"/> </div> </div> <p><br/><br/></p> <p>Take Voyager <a href="#2">[2]</a>, a lifelong learning agent in Minecraft, for example. It‚Äôs a lifelong learning agent in Minecraft which designs its own <em>curriculum</em>, builds a skill library and constantly expands it by interacting with the environment. Each skill is defined in form of executable code blocks and more complex skills are build by composing simpler skills. For e.g., to <code class="language-plaintext highlighter-rouge">combatZombie</code>, the agent needs to <code class="language-plaintext highlighter-rouge">craftStoneSword</code> and <code class="language-plaintext highlighter-rouge">craftShield</code>, which in turn requires skills like <code class="language-plaintext highlighter-rouge">mineWood</code> and <code class="language-plaintext highlighter-rouge">makeFurnace</code>. So how does it work? No points for guessing that the world knowledge of the LLMs come in handy. In contrast, predefining every possible skill and scenario in a classical planner would require extensive knowledge of both the domain and the environment. <br/><br/></p> <h3 id="2-handling-abstract-tasks">2. Handling Abstract Tasks<br/><br/></h3> <table> <tbody> <tr> <td align="center" valign="top" width="25%"> <video width="100%" controls=""> <source src="/assets/videos/organize_closet.mp4" type="video/mp4"/> Your browser does not support the video tag. </video> <br/><sub><b>Organize Closet</b></sub> </td> <td align="center" valign="top" width="25%"> <video width="100%" controls=""> <source src="/assets/videos/browse_internet.mp4" type="video/mp4"/> Your browser does not support the video tag. </video> <br/><sub><b>Browse Internet</b></sub> </td> <td align="center" valign="top" width="25%"> <video width="100%" controls=""> <source src="/assets/videos/turn_off_tv.mp4" type="video/mp4"/> Your browser does not support the video tag. </video> <br/><sub><b>Turn off TV</b></sub> </td> </tr> </tbody> </table> <p><br/><br/></p> <p>Furthermore, LLMs, the veritable know-it-alls, can instantly generate plans for just about any task you can think of, all in <em>zero-shot</em> <a href="#3">[3]</a>. Meanwhile, classical planning requires expertly formalized goal conditions in a <a href="https://planning.wiki/ref/pddl/problem">PDDL problem file</a>.<br/><br/></p> <h3 id="3-handling-partial-observability">3. Handling Partial Observability<br/><br/></h3> <div class="row"> <div class="col-sm-12 mt-3 mt-md-0"> <img src="/assets/img/partial-observability.png" width="70%" alt="Description of the image content" class="img-fluid rounded z-depth-1" style="display: block; margin: auto;" onerror="this.onerror=null; this.src='image-not-found.png';"/> </div> </div> <p><br/><br/></p> <p>Classical planners generally work under the assumption of <em>full observability</em> modeled via a Markov Decision Process (MDP). Unfortunately, in the real-world, the agent‚Äôs senors provide only partial information, called <em>observation</em>. Such problems are modeled as a Partially Observable MDP (POMDP). At each time step, the sequence of observations made by the agent determines a probability distribution over states of the environment. Such a distribution is called a <em>belief state</em>. Needless to say that often leads to intractibility problems as the length of the plan grows. LLMs can use its world knowledge to handle partial observability. Think of it as a commonsense prior that helps to shape better (posterior) beliefs. For instance, given the task <code class="language-plaintext highlighter-rouge">make an omlette</code>, the LLM can output actions to <code class="language-plaintext highlighter-rouge">go to the fridge</code> since eggs are likely to be found in the fridge ‚Äì even though eggs are not visible (i.e. partial observation) <a href="#4">[4]</a>. <br/><br/></p> <div class="row"> <div class="col-sm-12 mt-3 mt-md-0"> <img src="/assets/img/partial-observability-2.png" width="70%" alt="Description of the image content" class="img-fluid rounded z-depth-1" style="display: block; margin: auto;" onerror="this.onerror=null; this.src='image-not-found.png';"/> </div> </div> <p><br/><br/></p> <p>Yet another example is where the agent is given the task of <code class="language-plaintext highlighter-rouge">stacking the lighter block on the heavier block</code><a href="#5">[5]</a>. Without the information of weights, a planner will never work. However, a LLM uses <em>active perception</em> to first pick up each block to register the weights, then perform the task. <br/><br/></p> <hr/> <h2 id="q2-are-they-really-all-you-need-for-planning">Q2. Are they really All You Need <em>for planning</em>?<br/><br/></h2> <h3 id="what-we-know-so-far">What we know so far?<br/><br/></h3> <table> <tr> <th></th> <td><strong>LLM Planning</strong></td> <td><strong>Classical Planning</strong></td> </tr> <tr> <td><strong>Open-world Planning</strong></td> <td align="center">‚úÖ</td> <td align="center">‚ùå</td> </tr> <tr> <td><strong>Handling Abstract Tasks</strong></td> <td align="center">‚úÖ</td> <td align="center">‚ùå</td> </tr> <tr> <td><strong>Handling Partial Observability</strong></td> <td align="center">‚úÖ</td> <td align="center">‚ùå</td> </tr> </table> <p><br/><br/></p> <p>Note that these aspects are not outliers ‚Äì on the contrary ‚Äì these are pretty much the norm in the real-world. With LLMs checking all the boxes, it is tempting to declare, <strong>‚ÄúAn LLM is all you need for planning. Period.‚Äù</strong> <br/><br/></p> <div class="row"> <div class="col-sm-12 mt-3 mt-md-0"> <img src="/assets/img/well-well.png" width="50%" alt="Description of the image content" class="img-fluid rounded z-depth-1" style="display: block; margin: auto;" onerror="this.onerror=null; this.src='image-not-found.png';"/> </div> ¬© DALL-E </div> <p><br/><br/></p> <p>But let‚Äôs hold that thought. The world of planning doesn‚Äôt end with these three aspects. Are LLMs the be-all and end-all for planning? Maybe not just yet. Let‚Äôs explore this further and keep our tech enthusiasm in check üòâ. <br/><br/></p> <h3 id="llms-seem-to-struggle-a-bit-6">LLMs seem to struggle a bit <a href="#6">[6]</a>.<br/><br/></h3> <div class="row"> <div class="col-sm-12 col-md-6 offset-md-3 mt-3 mt-md-0"> <video class="img-fluid rounded z-depth-1" controls="" style="display: block; margin-left: auto; margin-right: auto;"> <source src="/assets/videos/say_final_hanoi.mp4" type="video/mp4"/> Your browser does not support the video tag. </video> </div> </div> <p><br/><br/></p> <p>In the tower of Hanoi setup with just three disks, the LLM outputs infeasible actions.<br/><br/></p> <h3 id="in-fact-it-struggles-a-lot-6">In fact, it struggles a lot <a href="#6">[6]</a>.<br/><br/></h3> <div class="row"> <div class="col-sm-12 col-md-6 offset-md-3 mt-3 mt-md-0"> <video class="img-fluid rounded z-depth-1" controls="" style="display: block; margin-left: auto; margin-right: auto;"> <source src="/assets/videos/say_final_virtualhome.mp4" type="video/mp4"/> Your browser does not support the video tag. </video> </div> </div> <p><br/><br/></p> <p>Given the task to <code class="language-plaintext highlighter-rouge">browse internet</code>, the LLM generates actions like <code class="language-plaintext highlighter-rouge">walk stereo</code> and <code class="language-plaintext highlighter-rouge">find bed</code> which are irrelevant to the goal.<br/><br/></p> <h3 id="even-on-simple-blocksworld-7">Even on simple Blocksworld <a href="#7">[7]</a>.<br/><br/></h3> <div style="text-align: center; width: 50%; margin: auto;"> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Forget Super Bowl, it is more fun to watch the all powerful ChatGPT (v. Jan 30) trying to &quot;plan&quot; a 3 blocks configuration.. <br/><br/>tldr; LLM&#39;s are multi-shot &quot;apologetic&quot; planners that would rather use you as their world model cum debugger.. <a href="https://t.co/eYIeEVBJli">pic.twitter.com/eYIeEVBJli</a></p>&mdash; Subbarao Kambhampati (‡∞ï‡∞Ç‡∞≠‡∞Ç‡∞™‡∞æ‡∞ü‡∞ø ‡∞∏‡±Å‡∞¨‡±ç‡∞¨‡∞æ‡∞∞‡∞æ‡∞µ‡±Å) (@rao2z) <a href="https://twitter.com/rao2z/status/1624881790212251649?ref_src=twsrc%5Etfw">February 12, 2023</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </div> <p><br/><br/></p> <h3 id="and-neither-finetuning8-nor-clever-prompting9-helps">And neither Finetuning<a href="#8">[8]</a>, nor clever Prompting<a href="#9">[9]</a> helps.<br/><br/></h3> <ul> <li>On simple blocksworld, <u>finetuned GPT-3</u> could only solve 122/600 (20%)<a href="#8">[8]</a>.</li> <li>GPT-4 could only solve 210/600 (34%) in zero-shot and 214/600 (35.6%) with Chain-of-Thought (CoT) prompting<a href="#8">[8]</a>.</li> <li>LLMs perform poorly at verifying solutions (graph colorings<a href="#9">[9]</a>), hence showing they cannot self-critique and improve.</li> </ul> <p>Comparing to human performance:</p> <ul> <li>A group of 50 human planners were tested.</li> <li>39 (78%) came up with valid plans.</li> <li>35 (70%) came up with optimal plans.</li> </ul> <p>So what do we have now?<br/><br/></p> <table> <tr> <th></th> <td><strong>LLM Planning</strong></td> <td><strong>Classical Planning</strong></td> </tr> <tr> <td><strong>Open-world Planning</strong></td> <td align="center">‚úÖ</td> <td align="center">‚ùå</td> </tr> <tr> <td><strong>Handling Abstract Tasks</strong></td> <td align="center">‚úÖ</td> <td align="center">‚ùå</td> </tr> <tr> <td><strong>Handling Partial Observability</strong></td> <td align="center">‚úÖ</td> <td align="center">‚ùå</td> </tr> <tr> <td><strong>Feasibility</strong></td> <td align="center">‚ùå</td> <td align="center">‚úÖ</td> </tr> <tr> <td><strong>Optimality</strong></td> <td align="center">‚ùå</td> <td align="center">‚úÖ</td> </tr> </table> <p><br/><br/></p> <p><strong>LLMs are useful for planning. However, there are no formal guarantees.<br/><br/></strong></p> <hr/> <h2 id="is-it-possible-to-enhance-llms-to-have-some-formal-guarantees">Is it possible to enhance LLMs to have <u>some</u> formal guarantees?</h2> <p><br/><br/> Find out in the next installment of our series: <a href="https://rishihazra.github.io/llm-planning/2024/05/26/fusing_llms_and_planners.html">Fusing LLM and Classical Planning</a> <br/><br/></p> <div class="row"> <div class="col-sm-12 mt-3 mt-md-0"> <img src="/assets/img/Robot_Suspense_PNG.png" width="40%" alt="Description of the image content" class="img-fluid rounded z-depth-1" style="display: block; margin: auto;" onerror="this.onerror=null; this.src='image-not-found.png';"/> </div> ¬© DALL-E </div> <p><br/><br/></p> <hr/> <h2 id="references">References</h2> <p><a id="1">[1]</a> Ding, Y. (2023). Integrating action knowledge and LLMs for Task Planning and situation handling in open worlds. Autonomous Robots, 47, 981-997.</p> <p><a id="2">[2]</a> Wang, G. (2024). Voyager: An Open-Ended Embodied Agent with Large Language Models. Transactions on Machine Learning Research, 2835-8856.</p> <p><a id="3">[3]</a> Huang, W. (2022). Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. Proceedings of the 39th International Conference on Machine Learning, PMLR 162:9118-9147.</p> <p><a id="4">[4]</a> Vemprala, S. (2023). ChatGPT for Robotics: Design Principles and Model Abilities.</p> <p><a id="5">[5]</a> Sun, L. (2023). Interactive Planning Using Large Language Models for Partially Observable Robotics Tasks.</p> <p><a id="6">[6]</a> Hazra, R. (2023). SayCanPay: Heuristic Planning with Large Language Models Using Learnable Domain Knowledge. Proceedings of the AAAI Conference on Artificial Intelligence, 20123‚Äì20133.</p> <p><a id="7">[7]</a> Valmeekam, K. (2023). PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change. NeurIPS 2023 Track on Datasets and Benchmarks.</p> <p><a id="8">[8]</a> Kambhampati, S. (2023). On the Role of Large Language Models in Planning. ICAPS 2023 Tutorial</p> <p><a id="9">[9]</a> Stechly, K. (2023). GPT-4 Doesn‚Äôt Know It‚Äôs Wrong: An Analysis of Iterative Prompting for Reasoning Problems. NeurIPS 2023 Foundation Models for Decision Making Workshop</p>]]></content><author><name></name></author><category term="llm-planning"/><category term="LLMs"/><category term="Planning"/><summary type="html"><![CDATA[In my talks, I often encounter a couple of recurring questions. The first is Why do I use LLMs as Planners in my works? Is it simply because they‚Äôre the latest rage. Let‚Äôs be clear ‚Äì if you can do something with classical planners ‚Äì don‚Äôt use a trillion parameter model for the same? That‚Äôd be like throwing a kitchen sink at it. However, as we will see, not everything can be simply solved using a classical planner.]]></summary></entry><entry><title type="html">Fusing LLM &amp;amp; Classical Planning</title><link href="https://rishihazra.github.io/llm-planning/2024/05/26/fusing_llms_and_planners.html" rel="alternate" type="text/html" title="Fusing LLM &amp;amp; Classical Planning"/><published>2024-05-26T14:37:00+00:00</published><updated>2024-05-26T14:37:00+00:00</updated><id>https://rishihazra.github.io/llm-planning/2024/05/26/fusing_llms_and_planners</id><content type="html" xml:base="https://rishihazra.github.io/llm-planning/2024/05/26/fusing_llms_and_planners.html"><![CDATA[<p>In the previous part (<a href="https://rishihazra.github.io/llm-planning/2024/05/26/llms_as_planners.html">Why are LLMs required for Planning?</a>), the question we asked was: <strong>Can we improve LLM planning to have <u>some</u> formal guarantees?</strong></p> <p>To answer that, Let‚Äôs first recap.</p> <style>.blue-text{color:cornflowerblue}.pink-text{color:darksalmon}</style> <table> <tr> <th></th> <th class="pink-text">LLM Planning</th> <th class="blue-text">Classical Planning</th> </tr> <tr> <td><strong>Open-world Planning</strong></td> <td align="center">‚úÖ</td> <td align="center">‚ùå</td> </tr> <tr> <td><strong>Handling Abstract Tasks</strong></td> <td align="center">‚úÖ</td> <td align="center">‚ùå</td> </tr> <tr> <td><strong>Handling Partial Observability</strong></td> <td align="center">‚úÖ</td> <td align="center">‚ùå</td> </tr> <tr> <td><strong>Feasibility</strong></td> <td align="center">‚ùå</td> <td align="center">‚úÖ</td> </tr> <tr> <td><strong>Optimality</strong></td> <td align="center">‚ùå</td> <td align="center">‚úÖ</td> </tr> </table> <p><br/><br/></p> <p>We can observe that the formal guarantees come from classical planning. Can we somehow combine the best of both? Any guesses? Here‚Äôs a hint to jog your memories.<br/><br/></p> <div class="row"> <div class="col-sm-12 mt-3 mt-md-0"> <img src="/assets/img/thinking_fast_slow.png" width="40%" alt="Description of the image content" class="img-fluid rounded z-depth-1" style="display: block; margin: auto;" onerror="this.onerror=null; this.src='image-not-found.png';"/> </div> </div> <p><br/><br/></p> <p>Ring any bells now? You guessed it right. <br/><br/></p> <p>Considering <span style="color: darksalmon;">LLM Planning</span> as <span style="color: darksalmon;">System 1</span> and <span style="color: cornflowerblue;">Classical Planning</span> as <span style="color: cornflowerblue;">System 2</span>, we look at two different ways of combining the best of both.</p> <ul> <li>Equip LLMs with verifiers, dynamics, and heuristic search.</li> <li>Use LLms as knowledge bases for open-world planning <br/><br/></li> </ul> <div class="row"> <div class="col-sm-12 mt-3 mt-md-0"> <img src="/assets/img/classical_plus_llms.png" width="70%" alt="Description of the image content" class="img-fluid rounded z-depth-1" style="display: block; margin: auto;" onerror="this.onerror=null; this.src='image-not-found.png';"/> </div> </div> <p><br/><br/></p> <hr/> <h2 id="llms--verifiers-dynamics-and-heuristic-search">LLMs + verifiers, dynamics, and heuristic search.<br/><br/></h2> <div class="row"> <div class="col-sm-12 mt-3 mt-md-0"> <img src="/assets/img/verify_step_by_step.png" width="70%" alt="Description of the image content" class="img-fluid rounded z-depth-1" style="display: block; margin: auto;" onerror="this.onerror=null; this.src='image-not-found.png';"/> </div> </div> <p><br/><br/></p> <p>Let‚Äôs first start with some motivation. <a href="#1">[1]</a> shows that:</p> <ul> <li>LLMs can generate <em>good</em> solutions if called multiple times.</li> <li>Verifiers help discard <em>bad</em> candidate actions/plans.<br/><br/></li> </ul> <h3 id="1-llms--external-verifiers">1. LLMs + external verifiers<br/><br/></h3> <div class="row"> <div class="col-sm-12 mt-3 mt-md-0"> <img src="/assets/img/saycan.png" width="70%" alt="Description of the image content" class="img-fluid rounded z-depth-1" style="display: block; margin: auto;" onerror="this.onerror=null; this.src='image-not-found.png';"/> </div> </div> <p><br/><br/></p> <div class="row"> <div class="col-sm-12 mt-3 mt-md-0"> <img src="/SayCanPay/resources/saycanpay_teaser.png" width="80%" alt="Description of the image content" class="img-fluid rounded z-depth-1" style="display: block; margin: auto;" onerror="this.onerror=null; this.src='image-not-found.png';"/> </div> </div> <p><br/><br/></p> <p>SayCan has a critical limitation: it selects actions based solely on feasibility, not their relevance to the goal. Consider this analogy: if you‚Äôre traveling from San Francisco to New York, would it make sense to fly via New Delhi simply because it‚Äôs feasible? To address this, <strong>SayCanPay</strong><a href="#3">[3]</a> further adds a Pay model to estimate the payoff of an action with respect to the goal. That is, actions which are more <em>optimal</em> wrt the goal are more likely to be selected.<br/><br/></p> <h3 id="2-llms--external-verifiers--heuristic-search">2. LLMs + external verifiers + heuristic search<br/><br/></h3> <div class="row"> <div class="col-sm-12 mt-3 mt-md-0"> <img src="/SayCanPay/resources/decoding_strategies.png" width="80%" alt="Description of the image content" class="img-fluid rounded z-depth-1" style="display: block; margin: auto;" onerror="this.onerror=null; this.src='image-not-found.png';"/> </div> </div> <p><br/><br/></p> <p><strong>SayCanPay</strong><a href="#3">[3]</a> also proposes heuristic search using an aggregated score of the Say (LLM), Can (feasibility), and Pay (optimality) models. As shown in the Figure, the Beam-Action search performs a beam search over the action. This mirrors the search in heuristic planners. They show that the overall score for each action is a sum of the aggregated score and heuristic score, akin to A* planning. <br/><br/></p> <h3 id="3-llms--dynamics-models">3. LLMs + dynamics models<br/><br/></h3> <div class="row"> <div class="col-sm-12 mt-3 mt-md-0"> <img src="/assets/img/llms_as_world_models.png" width="80%" alt="Description of the image content" class="img-fluid rounded z-depth-1" style="display: block; margin: auto;" onerror="this.onerror=null; this.src='image-not-found.png';"/> </div> </div> <p><br/><br/></p> <div class="row"> <div class="col-sm-12 mt-3 mt-md-0"> <img src="/assets/img/video_language_planning.png" width="80%" alt="Description of the image content" class="img-fluid rounded z-depth-1" style="display: block; margin: auto;" onerror="this.onerror=null; this.src='image-not-found.png';"/> </div> </div> <p><br/><br/></p> <hr/> <h2 id="llms-as-knowledge-base-for-planners">LLMs as Knowledge Base for Planners<br/><br/></h2> <div class="row"> <div class="col-sm-12 mt-3 mt-md-0"> <img src="/assets/img/llm_p.png" width="80%" alt="Description of the image content" class="img-fluid rounded z-depth-1" style="display: block; margin: auto;" onerror="this.onerror=null; this.src='image-not-found.png';"/> </div> </div> <p><br/><br/></p> <div class="row"> <div class="col-sm-12 mt-3 mt-md-0"> <img src="/assets/img/hierarchical_planning.png" width="80%" alt="Description of the image content" class="img-fluid rounded z-depth-1" style="display: block; margin: auto;" onerror="this.onerror=null; this.src='image-not-found.png';"/> </div> </div> <p><br/><br/></p> <hr/> <p>Perhaps, a more apt conlusion would be:</p> <div class="row"> <div class="col-sm-12 mt-3 mt-md-0"> <img src="/assets/img/llm-modulo.png" width="70%" alt="Description of the image content" class="img-fluid rounded z-depth-1" style="display: block; margin: auto;" onerror="this.onerror=null; this.src='image-not-found.png';"/> </div> </div> <p><br/><br/></p> <div class="row"> <div class="col-sm-12 mt-3 mt-md-0"> <img src="/assets/img/llm_solver_conclusion.png" width="90%" alt="Description of the image content" class="img-fluid rounded z-depth-1" style="display: block; margin: auto;" onerror="this.onerror=null; this.src='image-not-found.png';"/> </div> </div> <p><br/><br/></p> <hr/> <h2 id="references">References</h2> <p><a id="1">[1]</a> Lightman, H. (2023). Let‚Äôs Verify Step by Step.</p> <p><a id="2">[2]</a> Ahn, M. (2022). Do As I Can, Not As I Say: Grounding Language in Robotic Affordances. 6th Annual Conference on Robot Learning</p> <p><a id="3">[3]</a> Hazra, R. (2023). SayCanPay: Heuristic Planning with Large Language Models Using Learnable Domain Knowledge. Proceedings of the AAAI Conference on Artificial Intelligence, 20123‚Äì20133.</p> <p><a id="4">[4]</a> Hao, S. (2023). Reasoning with Language Models is Planning with World Models. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 8154‚Äì8173.</p> <p><a id="5">[5]</a> Du, Y. (2024). Video Language Planning. The Twelfth International Conference on Learning Representations.</p> <p><a id="6">[6]</a> Liu, B. (2023). LLM+P: Empowering Large Language Models with Optimal Planning Proficiency.</p> <p><a id="7">[7]</a> Wong, L. (2024). Learning Grounded Action Abstraction from Language. The Twelfth International Conference on Learning Representations</p>]]></content><author><name></name></author><category term="llm-planning"/><category term="LLMs"/><category term="Planning"/><summary type="html"><![CDATA[In the previous part (Why are LLMs required for Planning?), the question we asked was: Can we improve LLM planning to have some formal guarantees?]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://rishihazra.github.io/2022/04/23/displaying-external-posts-on-your-al-folio-blog.html" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://rishihazra.github.io/2022/04/23/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://rishihazra.github.io/2022/04/23/displaying-external-posts-on-your-al-folio-blog.html"><![CDATA[]]></content><author><name></name></author></entry></feed>